version: "3.8"
services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "5174:80"
    volumes:
      - .:/app
    environment:
      - NODE_ENV=development
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost" ]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.50'
          memory: 512M

  app-dev:
    image: node:20-alpine
    working_dir: /app
    volumes:
      - .:/app
    command: sh -c "npm install && npm run dev"
    ports:
      - "5173:8080"
    environment:
      - NODE_ENV=development
      - VITE_SITE_URL=http://localhost:5173
    restart: unless-stopped

  tts-server:
    image: ghcr.io/coqui-ai/tts
    ports:
      - "5002:5002"
    volumes:
      - tts_models:/root/.local/share/tts
    gpus:
      - driver: nvidia
        count: all
    # Override entrypoint to call python3 directly
    entrypoint:
      - python3
      - TTS/server/server.py
    # Now give all of your server flags as a YAML list
    command:
      - --model_name
      - tts_models/en/ljspeech/tacotron2-DDC_ph
      - --use_cuda
      - "true"
    restart: unless-stopped
volumes:
  tts_models:


